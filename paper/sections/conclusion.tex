%!TEX root = ../data-imputation.tex
\section{Conclusions}
\label{sec:conclusion}
%
To the best of our knowledge, this is the first benchmark that compares classical and modern imputation approaches on a large number of datasets under realistic missingness conditions with respect to the imputation quality and the impact on the predictive performance of a downstream ML model. We also evaluated how the results changed when the imputation and downstream model were trained on incomplete data.

Our results can be summarized in two main findings. First, we demonstrate that imputation helps to increase the downstream predictive performance substantially regardless of the missingness conditions. When training data is fully observed, improvements for classification tasks were in more than 75\% of the cases between $10\%$ and $20\%$ and for regression tasks around $15\%$.

Second, we find that in almost all experiments, random forest-based imputation achieves the best imputation quality, and consequently also on the most data sets the best improvements on the downstream predictive performance. This finding is in line with previous imputation benchmark research in more constrained experimental conditions see also Section \ref{sec:related_work}. Yet, some aspects of these results appear at odds with some recent work on deep learning methods. While we are aware of the limitations of our experiments, see also Section \ref{sec:limitations}, we still argue that to better assess the value of most deep learning methods, it is helpful to stress test these methods under realistic conditions in large unified benchmarks with heterogeneous data sets \citep{Sculley2018, Bender2021}.
