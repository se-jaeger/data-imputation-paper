%!TEX root = ../data-imputation.tex
\section{Conclusions}
\label{sec:conclusion}
%
To the best of our knowledge, this is the first benchmark that compares classical and modern imputation approaches on a large number of datasets under realistic missingness conditions with respect to the imputation quality and the impact on the predictive performance of a downstream ML model. We also evaluated how the results change when the imputation and downstream model were trained on incomplete data.

Our results can be summarized in two main findings. First we demonstrate that imputation helps to increase the downstream predictive performance substantially regardless the missingness conditions. When training data is fully observed improvements for classification tasks were in more than 75\% of the cases between $10\%$ and $20\%$ and for regression tasks around $15\%$. 

Second we find that in almost all experiments, random forest-based imputation achieves the best imputation quality and consequently also on the most data sets the best improvements on the downstream predictive performance. This finding is in line with previous imputation benchmark research in more constrained experimental conditions, see also \autoref{sec:related_work}. Yet some aspects of these results appear at odds with some recent work on deep learning methods. While we are aware of the limitations of our experiments, see also \autoref{sec:limitations}, we still argue that in order to better assess the value of most deep learning methods it is helpful to stress test these methods under realistic conditions in large unified benchmarks with heterogeneous data sets \citep{Sculley2018, Bender2021}.
