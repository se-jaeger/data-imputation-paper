%!TEX root = ../data-imputation.tex

\section{Methods}
%
One of the main goals of this work is to provide a comprehensive evaluation of missing value imputation methods under realistic conditions. In particular we focus on two aspects a) a large suite of real-world data sets and tasks and b) realistic missingness patterns. The following sections describe the data sets we considered as well as the missingness patterns, followed by a detailed description of the imputation methods compared and the metrics used for the evaluation.

\subsection{Datasets}
%
We focus on a broad evaluation with several numeric datasets and tasks (regression, binary classification, and multi-class classification). The OpenML database\footnote{Website: \url{https://www.openml.org/}} contains thousands of datasets and provides an API to automatically download them. The Python package \code{scikit-learn} uses this API to retrieve the datasets and creates well-formed \code{DataFrames} that encode the tables' columns properly.

We filter available datasets as follows. To calculate the imputation performance, we need ground truth datasets that do not contain missing values. Further, especially deep learning models need sufficient data to learn their task properly. However, because we plan to run many experiments, the datasets must not be too big to keep training times feasible. For this reason, we choose datasets without missing values that contain 5 to 25 features and 3k to 100k observations. We then removed duplicated, corrupted, and Sparse ARFF\footnote{Attribute-Relation File Format} formatted datasets.

The resulting 69 datasets are composed of 21 regression, 31 binary classification, and 17 multi-class classification datasets. The supplementary material contains a detailed list of all datasets and further information, such as OpenML ID, name, and the number of observations and features.


\subsection{Missingness Patterns}
\label{sec:missingess_pattern}
Most research on missing value imputation considers three different types of missingness patterns:
%
\begin{itemize}
\item Missing completely at random (MCAR, see \autoref{tab:missingness_patterns_MCAR}): \\
Values are discarded independent of any other values
\item Missing at random (MAR, see \autoref{tab:missingness_patterns_MAR}): \\
Values in column $c$ are discarded dependent on values in another column $k\neq c$
\item Missing not at random (MNAR, see \autoref{tab:missingness_patterns_MNAR}): \\
Values in column $c$ are discarded dependent on their value in $c$
\end{itemize}
%
The most often used missingness pattern in the literature on missing value imputation is MCAR. Here the missing values are chosen independently at random. Usually the implementations of this condition draw a random number from a uniform distribution and discard a value if that random number was below the desired missingness ratio. Few studies report results on the more challenging conditions MAR and MNAR. We here aim for a realistic modelling of these missingness patterns inspired by observations in large scale real world data sets as investigated in \cite{Biessmann2018a}. We use an implementation proposed in \cite{Schelter2020a} and \cite{Jenga}, which selects two random percentiles of the values in a column, one for the lower and one for the upper bound of the value range considered. The range of the upper and lower bound depend on the desired fraction of values to be discarded. In the MAR condition, we discard values if values in a random other column fall in that percentile. In the MNAR condition we discard values in a column if the values themselves fall in that random percentile range.
%
\begin{table}
	\centering
%	\caption{
%		Examples of missingness patterns for a missingness ratio of 50\%. 	}
%	\label{tab:missingness_patterns}
%	\vspace{1em}
%	\begin{subtable}{0.3\textwidth}
\begin{minipage}{0.28\textwidth}
\centering
	\begin{tabular}{cc}
\toprule
 height &  height$_{\text{MCAR}}$ \\
\midrule
  179.0 &                     ? \\
  192.0 &                     ? \\
  189.0 &                 189.0 \\
  156.0 &                 156.0 \\
  175.0 &                     ? \\
  170.0 &                 170.0 \\
  181.0 &                     ? \\
  197.0 &                     ? \\
  156.0 &                 156.0 \\
  160.0 &                 160.0 \\
\bottomrule
\end{tabular}
\caption{
		Applying the MCAR condition to column \textit{height} discards five out of ten values independent of the height values.
	}
	\label{tab:missingness_patterns_MCAR}
\vspace{2em}
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}
\centering
	\begin{tabular}{ccc}
\toprule
 height & gender &  height$_{\text{MAR}}$ \\
\midrule
  200.0 &      m &                    ? \\
  191.0 &      m &                    ? \\
  198.0 &      f &                198.0 \\
  155.0 &      m &                    ? \\
  206.0 &      m &                    ? \\
  152.0 &      f &                152.0 \\
  175.0 &      f &                175.0 \\
  159.0 &      m &                    ? \\
  153.0 &      f &                153.0 \\
  209.0 &      m &                209.0 \\
\bottomrule
\end{tabular}
\caption{In the MAR condition \textit{height} values are discarded dependent on values in another column,  here \textit{gender}. All discarded \textit{height} values correspond to rows in which \textit{gender} was \textit{male}.
}
	\label{tab:missingness_patterns_MAR}
\end{minipage}
\hfill
\begin{minipage}{0.28\textwidth}
\centering
	\begin{tabular}{cc}
\toprule
 height &  height$_{\text{MNAR}}$ \\
\midrule
  154.0 &                     ? \\
  181.0 &                 181.0 \\
  207.0 &                 207.0 \\
  194.0 &                 194.0 \\
  153.0 &                     ? \\
  156.0 &                     ? \\
  198.0 &                 198.0 \\
  185.0 &                 185.0 \\
  155.0 &                     ? \\
  164.0 &                     ? \\
\bottomrule
\end{tabular}
\caption{In the MNAR condition \textit{height} values are discarded dependent on the actual \textit{height} values. All discarded values correspond to small \textit{height} values.
}
	\label{tab:missingness_patterns_MNAR}
\vspace{1em}
\end{minipage}

\end{table}


\subsection{Imputation Methods}
\label{sec:methods:impuation}
%
In this section, we describe our six single imputation methods. The overall goal of an imputer is to train a model on $X = [X_1, X_2, ..., X_{i-1}, X_{i+1}, ..., X_n]$, where $n$ is the number of features and $X_i$ the to-be-imputed (or target) column. Consequently, if there are $m$ columns with missing values, we need to train $m$ imputation models. However, to abstract this and other crucial steps, such as encode, normalize, and decode the data and cross-validate the imputer's hyperparameters, we define a framework implemented by all of our imputer implementations. As described in Section \ref{sec:introduction}, missing values can break ML pipelines. By default, we substitute missing values with column-wise mean/mode to prevent the framework from failing.


\subsubsection{Simple Imputer}
%
Our \code{Simple Imputer} uses the column-wise \code{mean} for numerical or \code{mode}, i.e., the most frequent value,  for categorical columns to fill missing values.


\subsubsection{Machine Learning Imputer}
%
We use two common imputation methods as representatives: $k$\emph{-NN Imputer} and \emph{Random Forest Imputer}. Both encode categorical features as one-hot columns and normalize the data by rescaling it to zero mean and unit variance. The imputer's hyperparameters are optimized by 5-fold cross-validated grid-search.

For our K-NN Imputer, we use, depending on the target columns' datatype (categorical or numerical), \code{scikit-learn}'s \code{KNeighborsClassifier} or \code{KNeighborsRegressor} and optimizes the $n\_neighbors \in \{1, 3, 5\}$  hyperparameter.

Similarly, the Random Forest Imputer uses the \code{RandomForestClassifier} or \code{RandomForestRegressor} and optimizes the hyperparameter $n\_estimators \in \{10, 50, 100\}$.


\subsubsection{Deep Learning Imputer}
%
Already very simple deep learning models can achieve good imputation results (TODO: cite). To easily optimize the model's architecture, we use the AutoML\footnote{"automated machine learning (AutoML) [...] automatically set [the model's] hyperparameters to optimize performance" \cite{AutoML}} library \code{autokeras} \citep{AutoKeras} to build our \emph{Deep Learning Imputer}.

For categorical columns, we use AutoKeras' \code{StructuredDataClassifier} and for numerical columns \code{StructuredDataRegressor}. Both classes take care of properly encode the data and optimize the model's architecture and hyperparameters. To reduce the training time, we change the maximum number of trials to $50$, which means \code{autokeras} tries 50 different model architecture and hyperparameter combinations, and the maximal number or of $epochs$ (\code{autokeras} uses early stopping) to 50.


\subsubsection{Generative Imputer}
%
Several generative models are successfully applied to data imputation, especially variational autoencoders (VAE) \citep{HIVAE, VAE_for_genomic_data, VAEM} and generative adversarial networks (GAN) \citep{GAIN, VIGAN, MisGAN}. VAEs learn to encode their input into a distribution over the latent space and decode by sampling from this distribution \citep{VAE}. On the other hand, GANs consist of two parts \citep{GAN}: a generator and a discriminator. In an adversarial process, the generator learns to generate samples that are as close as possible to the data distribution, and the discriminator learns to distinguish whether an example is true or generated.

\cite{GAIN} proposed Generative Adversarial Imputation Nets (GAIN) that adapts the GAN architecture as follows. GAIN takes as input some data $X$, encodes categorical columns as numbers from $0$ to $n-1$, where $n$ is the number of categories, and calculates a binary mask matrix $M$ that represents missing values. To normalize $X$ into $\bar{X}$, it first scales the data min-max ($0, 1$) and second replaces missing values with random uniform noise ($\{0, 0.01\}$). The generator learns to output $\hat{X}$, where initially missing values are replaced, based on its input $\bar{X}$ and $M$. The discriminator learns to reconstruct the mask $M$ based on generator's output $\hat{X}$ and a hint matrix $H$, using the hyperparameter $hint\_rate$, that provides the discriminator with information about $M$.

GAIN is optimized by minimizing the sum of the generator's loss and the with $\alpha$ weighted discriminator's loss, see \cite{GAIN} for details. Besides the learning rates, for the generator and discriminator, GAIN introduces two new hyperparameters we optimize $hint\_rate$ and $\alpha$. For this, we use 3-fold cross-validated grid-search of: $generator\_learning\_rate \in \{0.0001, 0.0005\}$, $discriminator\_learning\_rate \in \{0.00001, 0.00005\}$, $\alpha \in \{1, 10\}$, and $hint\_rate \in \{0.7, 0.9\}$.

Our \emph{VAE Imputer} is based on the by \cite{VAE} proposed architecture. We treat the number of hidden layers $n\_hidden\_layers$ for encoder and decoder as hyperparameter. The size of these layers, i.e., the number of neurons, are fix and set relatively to the input dimension, i.e., the data set's number of columns. The dimensionality of the hidden layer one is $50\%$, hidden layer two $30\%$, and latent space is $20\%$ of the input dimension.
To encode the categorical columns and replace missing values with random noise (\cite{CaminoVAE} already present good results with this approach), we use the same preprocessing strategy as for the \emph{GAIN Imputer}.
To optimize the hyperparameter $n\_hidden\_layers \in \{0, 1, 2\}$, we use 3-fold cross-validated grid-search.

The \emph{GAIN Imputer} as well as the \emph{VAE Imputer} are trained using Adam optimizer with default hyperparameters for max $50$ epochs (early stopping) and batch size of $64$.


\subsection{Evaluation Metrics}
%
To evaluate our experiments, we use two metrics. For regression downstream tasks or numerical imputation, we use the $RMSE$, shown in Equation \ref{eq:RMSE}, and for classification downstream tasks or categorical imputation the $macro\ F1$, shown in Equation \ref{eq:F1}.
%
\begin{equation}
	RMSE = \sqrt{\frac{1}{N} \sum_{i = 0}^{N} (x_i - \hat{x_i})^2}
	\label{eq:RMSE}
\end{equation}
%
\begin{equation}
	macro\ F1 = \sum_{i = 0}^{N} F1_i\text{, where }F1 = \frac{TP}{TP + \frac{1}{2}(FP + FN)}
	\label{eq:F1}
\end{equation}
%
Because $RMSE$ measures an error, the lower its value the better the performance. On the other hand, $F1$ (we use $macro\ F1$ and $F1$ synonym) calculates a score and, therefore, greater $F1$ means better performance.
