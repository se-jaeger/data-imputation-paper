%!TEX root = ../data-imputation.tex

\section{Methods}
\label{sec:methods}
%
One of the main goals of this work is to provide a comprehensive evaluation of missing value imputation methods under realistic conditions. In particular we focus on two aspects a) a large suite of real-world data sets and tasks and b) realistic missingness patterns. The following sections describe the data sets we considered as well as the missingness patterns, followed by a detailed description of the imputation methods compared and the metrics used for the evaluation.

\subsection{Datasets}
\label{sec:datasets}
%
We focus on a broad evaluation with several numeric datasets and tasks (regression, binary classification, and multi-class classification). The OpenML database\footnote{Website: \url{https://www.openml.org/}}~\citep{OpenML2013} contains thousands of datasets and provides an API to automatically download them. The Python package \code{scikit-learn} \citep{scikit-learn} uses this API to retrieve the datasets and creates well-formed \code{DataFrames} that encode the tables' columns properly.

We filter available datasets as follows. To calculate the imputation performance, we need ground truth datasets that do not contain missing values. Further, especially deep learning models need sufficient data to learn their task properly. However, because we plan to run many experiments, the datasets must not be too big to keep training times feasible. For this reason, we choose datasets without missing values that contain 5 to 25 features and 3k to 100k observations. We then removed duplicated, corrupted, and Sparse ARFF\footnote{Attribute-Relation File Format} formatted datasets.

The resulting 69 datasets are composed of 21 regression, 31 binary classification, and 17 multi-class classification datasets. The supplementary material contains a detailed list of all datasets and further information, such as OpenML ID, name, and the number of observations and features.


\subsection{Missingness Patterns}
\label{sec:missingess_pattern}
Most research on missing value imputation considers three different types of missingness patterns:
%
\begin{itemize}
\item Missing completely at random (MCAR, see Table \ref{tab:missingness_patterns_MCAR}): \\
Values are discarded independent of any other values
\item Missing at random (MAR, see Table \ref{tab:missingness_patterns_MAR}): \\
Values in column $c$ are discarded dependent on values in another column $k\neq c$
\item Missing not at random (MNAR, see Table \ref{tab:missingness_patterns_MNAR}): \\
Values in column $c$ are discarded dependent on their value in $c$
\end{itemize}
%
The most often used missingness pattern in the literature on missing value imputation is MCAR. Here the missing values are chosen independently at random. Usually the implementations of this condition draw a random number from a uniform distribution and discard a value if that random number was below the desired missingness ratio. Few studies report results on the more challenging conditions MAR and MNAR. We here aim for a realistic modelling of these missingness patterns inspired by observations in large scale real world data sets as investigated in \cite{Biessmann2018a}. We use an implementation proposed in \cite{Schelter2020a} and \cite{Jenga}, which selects two random percentiles of the values in a column, one for the lower and one for the upper bound of the value range considered. The range of the upper and lower bound depend on the desired fraction of values to be discarded. In the MAR condition, we discard values if values in a random other column fall in that percentile. In the MNAR condition we discard values in a column if the values themselves fall in that random percentile range.
%
\begin{table}
	\centering
%	\caption{
%		Examples of missingness patterns for a missingness ratio of 50\%. 	}
%	\label{tab:missingness_patterns}
%	\vspace{1em}
%	\begin{subtable}{0.3\textwidth}
\begin{minipage}{0.28\textwidth}
\centering
	\begin{tabular}{cc}
\toprule
 height &  height$_{\text{MCAR}}$ \\
\midrule
  179.0 &                     ? \\
  192.0 &                     ? \\
  189.0 &                 189.0 \\
  156.0 &                 156.0 \\
  175.0 &                     ? \\
  170.0 &                 170.0 \\
  181.0 &                     ? \\
  197.0 &                     ? \\
  156.0 &                 156.0 \\
  160.0 &                 160.0 \\
\bottomrule
\end{tabular}
\caption{
		Applying the MCAR condition to column \textit{height} discards five out of ten values independent of the height values.
	}
	\label{tab:missingness_patterns_MCAR}
\vspace{2em}
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}
\centering
	\begin{tabular}{ccc}
\toprule
 height & gender &  height$_{\text{MAR}}$ \\
\midrule
  200.0 &      m &                    ? \\
  191.0 &      m &                    ? \\
  198.0 &      f &                198.0 \\
  155.0 &      m &                    ? \\
  206.0 &      m &                    ? \\
  152.0 &      f &                152.0 \\
  175.0 &      f &                175.0 \\
  159.0 &      m &                    ? \\
  153.0 &      f &                153.0 \\
  209.0 &      m &                209.0 \\
\bottomrule
\end{tabular}
\caption{In the MAR condition \textit{height} values are discarded dependent on values in another column,  here \textit{gender}. All discarded \textit{height} values correspond to rows in which \textit{gender} was \textit{male}.
}
	\label{tab:missingness_patterns_MAR}
\end{minipage}
\hfill
\begin{minipage}{0.28\textwidth}
\centering
	\begin{tabular}{cc}
\toprule
 height &  height$_{\text{MNAR}}$ \\
\midrule
  154.0 &                     ? \\
  181.0 &                 181.0 \\
  207.0 &                 207.0 \\
  194.0 &                 194.0 \\
  153.0 &                     ? \\
  156.0 &                     ? \\
  198.0 &                 198.0 \\
  185.0 &                 185.0 \\
  155.0 &                     ? \\
  164.0 &                     ? \\
\bottomrule
\end{tabular}
\caption{In the MNAR condition \textit{height} values are discarded dependent on the actual \textit{height} values. All discarded values correspond to small \textit{height} values.
}
	\label{tab:missingness_patterns_MNAR}
\vspace{1em}
\end{minipage}

\end{table}

\subsection{Data Preprocessing}
\label{sec:preprocessing}
%
Data preprocessing is often an important part of ML pipelines to achieve good results (TODO: cite?). In general, we do the following three preprocessing steps for all imputation methods:
%
\begin{itemize}
	\item Encode: \\
		The data sets (see Section \ref{sec:datasets}) we use are of heterogeneous columns, thus, categorical columns need to be encoded into a numerical representation

	\item Handle missing values: \\
		To avoid the imputation model from failing, it is necessary to handle missing values

	\item Normalize: \\
		Re-scale the columns to a common range makes ML algorithms more stable.
\end{itemize}
%
Encoding and normalizing mechanisms are defined on the training set and equally applied on the test set. The actual techniques varying for discriminative (described in Section \ref{sec:simple_imputation} to \ref{sec:dl_imputation}) and generative (described in Section \ref{sec:generative_imputation}) imputation approaches.

For discriminative imputation approaches, we substitute all missing values in the predictor columns with their column-wise mean/mode value to prevent the model from failing. The categorical columns are one-hot encoded and numerical ones normalized to zero mean and unit variance.

For Generative imputation approaches, it is important to preserve the number of columns, which is why we encode the categories of categorical columns as values from $0$ to $n-1$, where $n$ is the number of categories. Then, missing values are replaced with random uniform noise from $0$ to $0.01$ and, finally, the data is min-max scaled ranging from $0$ to $1$.

\subsection{Imputation Methods}
\label{sec:methods:impuation}
%
In this section, we describe our six single imputation methods. The overall goal of an imputation method is to train a model on $\vec{X}\in\R^{n\times d} = [\vec{x}_1, \vec{x}_2, ..., \vec{x}_{i-1}, \vec{x}_{i+1}, ..., \vec{x}_d]$, where $d$ is the number of features, $n$ the number of observations, and $\vec{x}_i$ denotes the to-be-imputed (or target) column.
To abstract crucial steps such as preprocessing the data (see Section \ref{sec:preprocessing}) and cross-validate the imputation method's hyperparameters (see Section \ref{sec:HPO}), we define a framework implemented by all of the following imputation approaches.


\subsubsection{Simple Imputation}
\label{sec:simple_imputation}
%
As a simple imputation baseline, we use the column-wise \code{mean} for numerical or \code{mode}, i.e., the most frequent value,  for categorical columns to fill missing values.


\subsubsection{$k$-NN Imputation}
\label{sec:knKNN}
%
A popular ML imputation baseline is $k$-NN imputation, also known as Hot-Deck imputation~\citep{Batista2003}. For our implementation thereof, we use \code{scikit-learn}'s \code{KNeighborsClassifier} for a categorical target columns and \code{KNeighborsRegressor} for numerical columns, respectively.
%Both models expose the hyperparameter $k \in \{1, 3, 5\}$, which we optimize by 5-fold cross-validated grid-search.


\subsubsection{Random Forest Imputation}
%
Similarly to the $k$-NN Imputation approach, described in Section \ref{sec:knKNN}, we implement the random forest imputation method using \code{scikit-learn}'s \code{RandomForestClassifier} and \code{RandomForestRegressor}.

%A very important hyperparameter of the models \code{RandomForestClassifier} and \code{RandomForestRegressor} is $n\_estimators$. Again, we use  5-fold cross-validated grid-search to optimize $n\_estimators \in \{10, 50, 100\}$.


\subsubsection{Discriminate Deep Learning Imputation}
\label{sec:dl_imputation}
%
Often simple deep learning models can achieve good imputation results~\citep{Biessmann2018a}. To easily optimize the model's architecture, we use the AutoML\footnote{"automated machine learning (AutoML) [...] automatically set [the model's] hyperparameters to optimize performance" \cite{AutoML}} library \code{autokeras} \citep{AutoKeras} to implement the discriminate deep learning imputation method.
For categorical columns, we use AutoKeras' \code{StructuredDataClassifier} and for numerical columns \code{StructuredDataRegressor}. Both classes take care of properly encode the data themselves and optimize the model's architecture and hyperparameters. We use $max\_trials = 50$, which means AutoKeras tries up to $50$ different model architecture and hyperparameter combinations, and $epochs = 50$ such that each model is trained for maximum $50$ epochs (AutoKeras uses early stopping by default).


\subsubsection{Generative Deep Learning Imputation}
\label{sec:generative_imputation}
%
All of the above approaches essentially follow the ideas known in the statistics literature as multiple imputation with chained equations (MICE) \citep{Little} or as {\em fully conditional specification} \citep{vanBuuren2018}: a discriminative model is trained on all but one column as features and the remaining column as the target variable. This approach has the advantage to be applicable to any supervised learning method, but it has the decisive disadvantage that for each to-be-imputed column a new model has to be trained. Generative approaches are different in that they train just one model for an entire table. All matrix factorization based approaches such as \citep{Troyanskaya2001,Koren2009,Mazumder2010} can be thought of as an example of generative models for imputation. We do not consider those linear generative models here as they have been shown to be outperformed by the above mentioned methods and focus on deep learning variants of generative models only.

Generative Deep Learning methods can be broadly categorized into classes, (variational) autoencoders (VAE)~\citep{VAE}\footnote{We focus on probabilistic autoencoders here as there are more imputation methods available for VAEs} and generative adversarial networks (GAN)~\citep{GAN}. In the following, we shortly highlight some representative imputation methods based on either of these two and describe the implementation used in our experiments.

\paragraph{Variational Autoencoder (VAE) Imputation}
%
VAEs learn to encode their input into a distribution over the latent space and decode by sampling from this distribution \citep{VAE}. Imputation methods based on this type of generative model include \cite{HIVAE, VAE_for_genomic_data, VAEM}. Rather than comparing all existing implementations we here focus on the original VAE imputation method for the sake of comparability with other approaches. To find the best model architecture, i.e., number of hidden layers and their sizes, we follow the approach proposed by \cite{CaminoVAE}. We test using no, one, and two hidden layers for the encoder and decoder and fixed their sizes relatively to the input dimension, i.e., the table's number of columns. If existing, the encoder's first hidden layer has $50\%$ of the input layer's neurons and the second $30\%$. The decoder's sizes are vice versa for upsampling the information to the same size as the input data. The latent space is also fixed to $20\%$ of the input dimension.
For training, we use Adam optimizer with default hyperparameters, batch size of $64$, and early stopping within $50$ epochs.

%We use 3-fold cross-validated grid-search to find the best number of hidden layers $\{0, 1, 2\}$. The layers' sizes are fixed and determined relatively to the input dimension, i.e., the table's number of columns. The encoder's first hidden layer has $50\%$ of the input layer's neurons, the second $30\%$, and the last layer, i.e., the latent space, $20\%$. The decoder's hidden layers' sizes are vice versa for upsampling the information to the same size as the input data.
%To train the VAE, we use Adam optimizer with default hyperparameters, batch size of $64$, and early stopping within $50$ epochs.


%We treat the number of hidden layers $n\_hidden\_layers$ for encoder and decoder as hyperparameters\felix{Which values were chosen?}. The size of these layers, i.e., the number of neurons, are fix and set relatively to the input dimension, i.e., the data set's number of columns\felix{Why? Can we motivate that decision?}\sebastian{For NNs the input dimension is (almost?) always the number of features. (here columns of the table)}. The dimensionality of the hidden layer one is $50\%$, hidden layer two $30\%$, and latent space is $20\%$ of the input dimension.
%\felix{not sure this sentence is grammatically correct?} To encode the categorical columns and replace missing values with random noise \felix{what noise? uniform, normal? what mean/std?} (\cite{CaminoVAE} already present good results with this approach), we use the same preprocessing strategy as for the \emph{GAIN Imputer}.
%\felix{didn't we also have mean/0 imputation instead of noise?}
%To optimize the hyperparameter $n\_hidden\_layers \in \{0, 1, 2\}$, we use 3-fold cross-validated grid-search.

\paragraph{Generative Adversarial Network (GAN) Imputation}
%
GANs consist of two parts \citep{GAN}: a generator and a discriminator. In an adversarial process, the generator learns to generate samples that are as close as possible to the data distribution, and the discriminator learns to distinguish whether an example is true or generated. Imputation approaches based on GANs include \cite{GAIN, VIGAN, MisGAN}.
Here we employ one of the most popular approaches of GAN based imputation, Generative Adversarial Imputation Nets (GAIN)~\citep{GAIN}.
GAIN adapts the original GAN architecture as follows. The generator's input is the concatenation of the input data and a binary matrix that represents the missing values. The discriminator learns to reconstruct the mask matrix by concatenating the generator's output and a hint matrix that is based on the introduced hyperparameter $hint\_rate$ revealing partial information about the missingness of the original data. A second hyperparameter GAIN introduces $\alpha$ is used to balance the generators performance for observed and missing values.
For training, we use Adam optimizer with default hyperparameters except the learning rate for generator and discriminator, batch size of $64$, and early stopping withing $50$ epochs.

%To optimize these and others, we use 3-fold cross-validated grid-search of: $generator\_learning\_rate \in \{0.0001, 0.0005\}$, $discriminator\_learning\_rate \in \{0.00001, 0.00005\}$, $\alpha \in \{1, 10\}$, and $hint\_rate \in \{0.7, 0.9\}$.
%Other hyperparameters, we do not optimize are the usage of Adam optimizer for generator and discriminator, batch size of $64$, and early stopping withing $50$ epochs.



%\felix{if this is the only place where we used math notation, we might consider rephrasing this, none of the other methods is described in such detail.} GAIN takes as input some data $X$, encodes categorical columns as numbers from $0$ to $n-1$, where $n$ is the number of categories, and calculates a binary mask matrix $M$ that represents missing values. To normalize $X$ into $\bar{X}$, it first scales the data min-max ($0, 1$) and second replaces missing values with random uniform noise ($\{0, 0.01\}$). The generator learns to output $\hat{X}$, where initially missing values are replaced, based on its input $\bar{X}$ and $M$. The discriminator learns to reconstruct the mask $M$ based on generator's output $\hat{X}$ and a hint matrix $H$, using the hyperparameter $hint\_rate$, that provides the discriminator with information about $M$.

%GAIN is optimized by minimizing the sum of the generator's loss and the with $\alpha$ weighted discriminator's loss, see \cite{GAIN} for details. Besides the learning rates, for the generator and discriminator, GAIN introduces two new hyperparameters we optimize $hint\_rate$ and $\alpha$. For this, we use 3-fold cross-validated grid-search of: $generator\_learning\_rate \in \{0.0001, 0.0005\}$, $discriminator\_learning\_rate \in \{0.00001, 0.00005\}$, $\alpha \in \{1, 10\}$, and $hint\_rate \in \{0.7, 0.9\}$.


\subsection{Hyperparameter Optimization}
\label{sec:HPO}
%
Optimizing and cross-validate hyperparameters is crucial to gain insights of a models' performance, robustness, and training time. Therefore, we choose for each imputation model the, as we find, most important hyperparameters and optimize them using cross-validated grid-search. For the $k$-NN and random forest imputation methods, we use $5$-fold cross validation and, to reduce the overall training time, $3$-fold cross validation for VAE and GAIN imputation. The mean/mode imputation does not have hyperparameters to optimize and because we use AutoKeras for the discriminate deep learning Imputation we do not need to set the hyperparameters explicitly. Table \ref{tab:HPO} gives an overview of all imputation approaches and their hyperparameters we optimize as well as the number of combinations.
%
\begin{table}[]
	\centering
	\begin{tabular}{@{}llll@{}}
		\toprule
		\multirow{2}{*}{Imputation Method} & \multicolumn{2}{c}{Hyperparameters}                          & \multirow{2}{*}{Grid Size} \\
		\\[-0.75em]
		& \multicolumn{1}{c}{Name}        & \multicolumn{1}{c}{Values} &                            \\ \midrule
		Mean/Mode                         &                                 &                            & 0                          \\
		\\[-0.75em]
		$k$-NN                             & $n\_neighbors$                  & \{1, 3, 5\}                & 3                          \\
		\\[-0.75em]
		Random Forest                      & $n\_estimators$                 & \{10, 50, 100\}            & 3                          \\
		\\[-0.75em]
		Discriminate DL*                   &                                 &                            &                            \\
		\\[-0.75em]
		VAE                                & $n\_hidden\_layers$             & \{0, 1, 2\}                & 3                          \\
		\\[-0.75em]
		\multirow{4}{*}{GAIN}              & $alpha$                         & \{1, 10\}                  & \multirow{4}{*}{16}        \\
		& $hint\_rate$                    & \{0.7, 0.9\}               &                            \\
		& $generator\_learning\_rate$     & \{0.0001, 0.0005\}         &                            \\
		& $discriminator\_learning\_rate$ & \{0.00001, 0.00005\}       &                            \\ \midrule
		\multicolumn{4}{l}{* Optimized using AutoKeras see Section \ref{sec:dl_imputation}}
	\end{tabular}
	\caption{Overview of all imputation methods and their hyperparameters we optimized. \emph{Mean/Mode} imputation does not have any hyperparameters and \emph{Discriminate DL} is optimized using AutoKeras, which is why we do not explicitly define a hyperparameter grid.}
	\label{tab:HPO}
\end{table}



\subsection{Evaluation Metrics}
%
To evaluate our experiments, we use two metrics. For regression downstream tasks or numerical imputation, we use the $RMSE$, shown in Equation \ref{eq:RMSE}, and for classification downstream tasks or categorical imputation the $macro\ F1$, shown in Equation \ref{eq:F1}.
%
\begin{equation}
	RMSE = \sqrt{\frac{1}{N} \sum_{i = 0}^{N} (x_i - \hat{x_i})^2}
	\label{eq:RMSE}
\end{equation}
%
\begin{equation}
	macro\ F1 = \sum_{i = 0}^{N} F1_i\text{, where }F1 = \frac{TP}{TP + \frac{1}{2}(FP + FN)}
	\label{eq:F1}
\end{equation}
%
Because $RMSE$ measures an error, the lower its value the better the performance. On the other hand, $F1$ (we use $macro\ F1$ and $F1$ synonym) calculates a score and, therefore, greater $F1$ means better performance.
