%!TEX root = ../data-imputation.tex

\begin{abstract}
%
With the increasing importance and complexity of data pipelines for software applications, data quality became one of key challenges for researchers as well as practitioners. The importance of data quality has been recognised beyond the field of data engineering and data base management systems (DBMS): Also for machine learning (ML) applications high data quality standards are crucial to ensure robust predictive performance and responsible usage of automated decision making. One of the most frequent data quality problems are missing values. Incomplete data sets can break data pipelines and can have devastating impact on downstream ML applications when not detected. While statisticians and more recently ML researchers have introduced a variety of approaches to impute missing values, a comprehensive benchmark comparing classical and modern imputation approaches under fair and realistic conditions is missing. Here we aim to fill this gap. We conduct a comprehensive suite of experiments on a large number of data sets with heterogeneous data and realistic missingness conditions comparing both novel deep learning approaches and classical ML imputation methods. Each imputation method is evaluated with respect to the imputation quality but also with respect to the impact the imputation has on a downstream ML task. Our results provide valuable insights into the performance of a variety of imputation methods under realistic conditions and help to guide data preprocessing method selection for research as well as applications. 
	\keyFont{ \section{Keywords:} data quality, data cleaning, imputation, missing data, benchmark, real world data, MCAR, MNAR, MAR} %All article types: you may provide up to 8 keywords; at least 5 are mandatory.
\end{abstract}
