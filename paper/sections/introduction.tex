%!TEX root = ../data-imputation.tex
\section{Introduction}
\label{sec:introduction}

In recent years complex data pipelines have become a central component of many software systems. It has been widely recognized that monitoring and improving data quality in these modern software applications is an important challenge at the intersection of database management systems (DBMS) and machine learning (ML) \citep{Schelter2015,Abedjan2018}. A substantial part of the engineering efforts required for the maintenance of large-scale production systems, especially when ML components are involved, is dedicated to data quality \citep{Sculley2015,Bose2017b}.

Poor data quality can quickly break software applications and lead to application downtimes, often leading to significant economic costs. More importantly, poor data quality can also lead to automated decisions that are unfair, marginalizing minorities, or have other negative impacts on a societal level~\citep{Stoyanovich2020,Yang2020,Bender2021}. For this reason, many researchers have started investigating to what extent monitoring of data quality can be automated \citep{Abedjan2016,Baylor2017,Schelter2018,rukat2020towards}. While some aspects of such monitoring, like the consistency of data types, are easy to automate, others, like semantic correctness\footnote{A great example from life sciences is given in~\citep{Ziemann2016}}, are still the subject of active research~\citep{biessmann2021automated}. However, even if automatic monitoring tools, such as those proposed in~\cite{Schelter2017}, would be used, a central challenge remains: How can we fix the data quality problems detected automatically?

One of the most frequent data quality problems is \emph{missing values}~\citep{Kumar}. Reasons for incomplete data are manifold: data might be accidentally not recorded, lost through application or transmission errors, intentionally not filled in by users, or result from data integration errors.
%
Throughout the past decades, researchers from different communities have been contributing to an increasingly large arsenal of methods to impute missing values. Statisticians laid the theoretical foundations for missing value imputation \citep{Rubin} by describing different missingness patterns (more details in Section \ref{sec:missingess_pattern}). Statistical approaches have been proposed to handle missing values \citep{Graham}. Simple strategies include dropping incomplete observations or replacing missing values with constant mathematically valid values. While this might be a reasonable solution to ensure robust functioning of data pipelines, such approaches often reduce the amount of available data for downstream tasks and, depending on the missingness pattern, might also bias downstream applications~\citep{Stoyanovich2020,Yang2020} and thus further decrease data quality \citep{Little, Graham}. Other approaches popular in the statistics literature use more sophisticated modeling, such as multivariate imputation by chained equations (MICE)~\citep{Little,vanBuuren2018}.

More recently also ML approaches are being increasingly used for imputation. Popular methods include k-nearest neighbors (KNN)~\citep{Batista2003}, matrix factorization~\citep{Troyanskaya2001,Koren2009,Mazumder2010}, random forest-based approaches~\citep{Stekhoven2012}, discriminative deep learning methods~\citep{Biessmann2018a} as well as generative deep learning methods~\citep{HIVAE,GAIN}.

Most imputation studies provide solid experimental evidence that the respective proposed method in the application setting investigated outperforms other competitor baselines. Yet, it remains difficult to assess which imputation method consistently performs best in a large spectrum of application scenarios and data sets under realistic missingness conditions. In particular, most studies do not report both imputation quality and the impact of the imputation on downstream ML applications.

In this paper, we aim at filling this gap. We benchmark a representative set of imputation methods on a large number of data sets under realistic missingness conditions with respect to imputation quality and the impact on the predictive performance of downstream ML models. For our experiments, we use $69$ fully observed data sets from \code{OpenML} \citep{OpenML2013} with numeric and categorical columns. Each data set is associated with a downstream ML task (binary classification, multi-class classification, and regression). We run experiments by artificially introducing varying fractions of missing values of the three missingness patterns (MCAR, MAR, and MNAR, see also Section \ref{sec:methods}). We then measure both the imputation accuracy and impact on downstream performance in two application scenarios: (a) missing values first appear in production, i.e., we train on complete data and corrupt (and impute) only test data, and (b) the training data has missing values, i.e., we train on corrupted data.

The rest of the paper is structured as follows: In Section \ref{sec:related_work}, we review the related work on imputation benchmarking efforts and continue in Section \ref{sec:methods} with an overview of the missingness conditions and imputation methods investigated in this study.
