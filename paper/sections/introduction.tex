%!TEX root = ../data-imputation.tex
\section{Introduction}
\label{sec:introduction}

In recent years, complex data pipelines have become a central component of many software systems. It has been widely recognized that monitoring and improving data quality in these modern software applications is an important challenge at the intersection of database management systems (DBMS) and machine learning (ML) \citep{Schelter2015,Abedjan2018}. A substantial part of the engineering efforts required for maintaining large-scale production systems is dedicated to data quality, especially when ML components are involved \citep{Sculley2015,Bose2017b}.

Poor data quality can quickly break software applications and cause application downtimes, often leading to significant economic costs. Moreover, poor data quality can foster unfair automated decisions, which marginalize minorities or have other negative societal impacts~\citep{Stoyanovich2020,Yang2020,Bender2021}. For this reason, many researchers started investigating to what extent monitoring of data quality can be automated \citep{Abedjan2016,Baylor2017,Schelter2018,rukat2020towards}. While some aspects of such monitoring, like the consistency of data types, are easy to automate, others, like semantic correctness\footnote{A great example from life sciences is given in~\citep{Ziemann2016}}, are still the subject of active research~\citep{biessmann2021automated}. However, even if automatic monitoring tools, such as those proposed in~\cite{Schelter2017}, would be used, a central challenge remains: How can we automatically fix the detected data quality issues?

One of the most frequent data quality problems is \emph{missing values}~\citep{Kumar}. Reasons for incomplete data are manifold: data might be accidentally not recorded, lost through application or transmission errors, intentionally not filled in by users, or result from data integration errors.
%
Throughout the past decades, researchers from different communities have been contributing to an increasingly large arsenal of methods to impute missing values. Statisticians laid the theoretical foundations for missing value imputation \citep{Rubin} by describing different missingness patterns (more details in Section \ref{sec:missingess_pattern}). Statistical approaches have been proposed to handle missing values \citep{Graham}. Simple strategies include dropping incomplete observations or replacing missing values with constant mathematically valid values. While this might be a reasonable solution to ensure robust functioning of data pipelines, such approaches often reduce the amount of available data for downstream tasks and, depending on the missingness pattern, might also bias downstream applications~\citep{Stoyanovich2020,Yang2020} and thus further decrease data quality \citep{Little, Graham}. Other approaches popular in the statistics literature use more sophisticated modeling, such as multivariate imputation by chained equations (MICE)~\citep{Little,vanBuuren2018}.

More recently also ML approaches are increasingly used for imputation. Popular methods include k-nearest neighbors ($k$-NN)~\citep{Batista2003}, matrix factorization~\citep{Troyanskaya2001,Koren2009,Mazumder2010}, random forest-based approaches~\citep{Stekhoven2012}, discriminative deep learning methods~\citep{Biessmann2018a} as well as generative deep learning methods~\citep{HIVAE,GAIN}.

Most imputation studies provide solid experimental evidence that the respective proposed method in the application setting investigated outperforms other competitor's baselines. Yet, it remains hard to assess which imputation method consistently performs best in a large spectrum of application scenarios and data sets under realistic missingness conditions. In particular, most studies do not report both imputation quality and the impact of the imputation on downstream ML applications.

In this paper, we aim at filling this gap. We benchmark a representative set of imputation methods on a large number of data sets under realistic missingness conditions with respect to imputation quality and the impact on the predictive performance of downstream ML models. For our experiments, we use $69$ fully observed data sets from \code{OpenML} \citep{OpenML2013} with numeric and categorical columns. Each data set is associated with a downstream ML task (binary classification, multi-class classification, and regression). We run experiments by artificially introducing varying fractions of missing values of the three missingness patterns (MCAR, MAR, and MNAR, see also Section \ref{sec:methods}). We then measure both the imputation accuracy and impact on downstream performance in two application scenarios: (a) missing values in the test data, i.e., we train on complete data, and corrupt (and impute) only test data, and (b) both training and test data have missing values, i.e., we train and test on corrupted data.


The rest of this paper is structured as follows. In Section \ref{sec:related_work}, we review the related work on imputation benchmarking efforts and continue in Section \ref{sec:methods} with an overview of the missingness conditions and imputation methods investigated in this study. A detailed description of our benchmark suite and its implementation follows in Section \ref{sec:implementation}. The results of our experiments are described and visualized in Section \ref{sec:results}. We then highlight the key findings in Section \ref{sec:discussion} and, finally, draw our conclusions in Section \ref{sec:conclusion}.
