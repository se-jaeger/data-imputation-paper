%!TEX root = ../data-imputation.tex
\section{Introduction}
\label{sec:introduction}

In recent years complex data pipelines have become a central component of many software systems. It has been widely recognised that monitoring and improving data quality in these modern software applications is an important challenge at the intersection of data base management systems (DBMS) and machine learning (ML) \citep{Schelter2015,Abedjan2018}. A substantial part of the engineering efforts required for maintenance of large scale production systems, especially when ML components are involved, is dedicated to data quality \citep{Sculley2015,Bose2017b}.

Poor data quality can quickly break software applications and lead to application downtimes, which often lead to significant economic cost. More importantly poor data quality can also lead to automated decisions that are unfair, marginalizing minorities or have other negative impact on a societal level~\citep{Stoyanovich2020,Yang2020,Bender2021}. This is why many researchers have started investigating to what extent monitoring of data quality can be automated \citep{Abedjan2016,Baylor2017,Schelter2018,rukat2020towards}. While some aspects of such monitoring, like consistency of data types, are easy to automate, others, like semantic correctness\footnote{A great example from life sciences is given in~\citep{Ziemann2016}}, are are still subject of active research~\citep{biessmann2021automated}. However even if automatic monitoring tools such as those proposed in~\citep{Schelter2017} would be used, a central challenge remains: How can we fix the data quality problems detected automatically?

One of the most frequent data quality problems are \emph{missing values}~\citep{Kumar}. Reasons for incomplete data are manifold: data might be accidentally not recorded, lost through application or transmission errors, intentionally not filled in by users, or result from data integration errors.
%
Throughout the past decades there researchers from different communities have been contributing to an increasingly large arsenal of methods to impute missing values. Statisticians laid the theoretical foundations for missing value imputation \citep{Rubin} by describing different missingness patterns (more details in Section \ref{sec:missingess_pattern}). Statistical approaches have been proposed to handle missing values \citep{Graham}. Simple strategies include dropping incomplete observations or replacing missing values with constant mathematically valid values. While this might be a reasonable solution to ensure robust functioning of data pipelines, such approaches often reduce the amount of available data for downstream tasks. And depending on the missingness pattern such approaches might also bias downstream applications~\citep{Stoyanovich2020,Yang2020} and thus further decrease data quality of the output of an application \citep{Little, Graham}. Other approaches popular in the statistics literature use more sophisticated modelling, such as multivariate imputation by chained equations (MICE)~\citep{Little,vanBuuren2018}.

More recently also ML approaches are being increasingly used for imputation. Popular approaches include k-nearest neighbors (KNN)~\citep{Batista2003}, matrix factorization~\citep{Troyanskaya2001,Koren2009,Mazumder2010}, random forest based approaches~\citep{Stekhoven2012}, discriminative deep learning methods~\citep{Biessmann2018a} as well as generative deep learning methods~\citep{HIVAE,GAIN}.

%With improvements of ML algorithms, methods to impute missing values evolved too. We define four different categories with the increasing complexity of imputation methods:
%%
%\begin{enumerate}
%	\item \textbf{Simple:} substitute missing values with the column-wise mean or mode
%	\item \textbf{Classical Machine Learning:} use a $k$-NN, random forest, logistic regression, or similar models
%	\item \textbf{Deep Learning:} use a neural network
%	\item \textbf{Generative:} use generative models, such as variational autoencoders or generative adversarial network
%\end{enumerate}
%

Most of the work on imputation provides solid experimental evidence that the respective proposed method in the application setting investigated outperforms other competitor baselines. Yet it remains difficult to assess which imputation method would consistently perform best in a large spectrum of application scenarios and data sets under realistic missingness conditions. In particular most studies do not report both imputation quality as well as the impact of the imputation on downstream ML applications.

In this paper, we aim at filling this gap. We benchmark a representative set of imputation methods on a large number of data sets under realistic missingness conditions with respect to imputation quality as well as impact on the predictive performance of downstream ML models. For our experiments use $69$ fully observed data sets from \code{OpenML} \citep{OpenML2013} with numeric and categorical columns. Each data set is associated with a downstream ML task (binary classification, multi-class classification, and regression). We run experiments by artificially introducing varying fractions of missing values of the three missingness patterns (MCAR, MAR and MNAR, see also. \autoref{sec:methods}). We then measure both the imputation accuracy and impact on downstream performance in two application scenarios: (a) missing values first appear in production, i.e., we train on complete data and corrupt (and impute) only test data, and (b) the training data has missing values, i.e., we train on corrupted (and imputed) data. \felix{Is this correct?}

The rest of the paper is structured as follows: In Section \ref{sec:related_work} we review the related work on imputation benchmarking efforts and continue in Section \ref{sec:methods} with an overview of the missingness conditions and imputation methods investigated in this study.
