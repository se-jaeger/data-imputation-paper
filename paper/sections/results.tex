%!TEX root = ../data-imputation.tex
\section{Results}
\label{sec:results}

In this section, we describe and visualize the results of our experiments. For the visualization we choose to consistently use boxplots for all four experiments/scenarios. These allow us to get a decent impression of the distribution of the results based on quantiles. In a line chart, in contrast, the confidence bands would overlap too much to derive meaningful interpretations. The plots' arrangement from left to right corresponds to the degree of difficulty increasing in this direction. This applies to the missingness patterns (MCAR, MAR, MNAR), each of which gets an own sub-plot, as well as the missingness percentages (small to larger), which are depicted as ticks on the x-axis. To show different effects of imputing categorical or numerical columns, we split the plots horizontally. Because we randomly sample on target column for each data set, there are about $13\%$ categorical ($9$) and $87\%$ numerical ($60$) columns. Respectively, for the second experiment, the horizontal split presents classification and regression downstream tasks, which are also imbalanced: $48$ classification ($\sim70\%$) and $21$ regression tasks ($\sim30\%$).



\subsection{Experiment 1: Imputation Quality}

In this experiment we evaluate the imputation performance of each method when training on complete data.

As described above, our goal was to provide a broad overview of the imputation methods' performance on various data sets. Using randomly sampled target columns on heterogeneous data lead to a wide range of values for their evaluation metric ($F1$/$RMSE$), which makes it difficult to compare. To solve this problem, we split the results in categorical and numerical imputation and compute the rank of each imputation method for every missingness pattern and fraction separately. Since we use six imputation methods, there are six ranks, where rank one is best and rank six the worst. If two or more methods perform equal, we assign the same rank and for methods that failed during training rank six.


\subsubsection{Scenario 1: Training on Complete Data}
\label{sec:results_experiment1_scenario1}

\begin{figure}\centering
    \includegraphics[width=1\columnwidth]{fully_observed_impute_rank_boxplot.eps}
    \caption[Imputation Ranks - Fully Observed]{Imputation ranks of the on complete data trained imputation methods. Ranks are computed for a given experimental setting and range between one and six. The box plots present the distribution of the imputation method's ranks over all cases. Rows distinguish categorical and numerical imputation, and columns the patterns: MCAR, MAR, and MNAR. For numerical imputation, clearly random forest closely followed by $k$-NN and discriminative DL are best. For categorical imputation, the results are less clear. However, the same three tend to be best for MCAR and MAR. For MNAR their performance degrades in favor for mean/mode imputation.
	}
	\label{fig:fully_observed_impute_rank_boxplot}
\end{figure}

%\sebastian{Unterbringen: If an imputation method did not work, it is ranked last. This is most relevant for GAIN, which failed in 273 out of 828 times in \textit{Experiment 1} - \textit{Scenario 1}, which amounts to 33\% of the cases. In \textit{Experiment 1} - \textit{Scenario 2}, however, the problem of GAIN often failing did not occur.}

Figure \ref{fig:fully_observed_impute_rank_boxplot} presents the on complete data trained imputation methods' ranks based on their imputation performance. In about $33\%$ of these results, GAIN failed during training and get assigned the worst rank six.

When imputing categorical columns, there is no clear best method. However, in many settings the discriminative DL approach achieves for $75\%$ of the cases at least rank three or better. Very similar but slightly worse results are shown by the random forest imputation method. For MCAR with $50\%$ missing values and MAR with $10\%$ to $50\%$ missingness, the $k$-NN imputation approach performs good and gets for $75\%$ of the cases at least rank three or better. VAE ranges in most settings for $50\%$ of the cases between rank two and four, and GAIN shows consistently bad performance and ranges in most settings for $75\%$ of the cases on rank four or worse. Interestingly, mean/mode imputation shows better ranks for the more complex settings with MNAR missingness pattern.

When imputing numerical columns the differences are more pronounced. Random forest is the only method that ranges for $75\%$ of the cases among the first three ranks throughout all experimental conditions. Also $k$-NN shows good results, ranging in most settings in $50\%$ of the cases on the second or third rank. Very similar results are shown from the discriminative DL method that tends to lose performance from MAR with $30\%$ missingness to MNAR with $50\%$ missing values. Again VAE ranges most of the time between rank three and five similar to mean/mode imputation and GAIN gets the worst ranks five and six.

To summarize, simple imputation methods, such as $k$-NN and random forest, often perform best, closely followed by the discriminative DL approach. However, for imputing categorical columns with MNAR missing values mean/mode imputation often performs well, especially for high fractions of missing values. The generative approaches get middle ranks (VAE) or range on the worst ranks (GAIN).


\subsubsection{Scenario 2: Training on Incomplete Data}


\begin{figure}\centering
    \includegraphics[width=1\columnwidth]{corrupted_impute_rank_boxplot.eps}
    \caption[Imputation Ranks - Corrupted]{Imputation ranks of the on incomplete data trained imputation methods. Ranks are computed for a given experimental setting and range between one and six. The box plots present the distribution of the imputation method's ranks over all cases. Rows distinguish categorical and numerical imputation, and columns the patterns: MCAR, MAR, and MNAR. For numerical imputation, clearly random forest closely followed by $k$-NN and discriminative DL are best. For numerical columns with MCAR and MAR missing values, $k$-NN and random forest are favorable. For MNAR, all approaches degrade in favor of mean/mode that clearly outperforms the other for $30\%$ and $50\%$ missingness.}
	\label{fig:corrupted_impute_rank_boxplot}
\end{figure}

\autoref{fig:corrupted_impute_rank_boxplot} shows the imputation performance in \textit{Scenario 2}, i.e. when training on incomplete data. Imputing categorical columns with increasing difficulty, the ranks of mean/mode imputation improves. From MCAR $30\%$ to MNAR $50\%$, $k$-NN is in $75\%$ of the cases on at least the third rank or better, often it ranges on the first and second rank. For MNAR its performance degrades gradually in favor of mean/mode that show surprinsingly good results, especially for the most challenging settings (MNAR with $30\%$ and $50\%$ missing values) where it clearly outperforms others in at least $75\%$ of the cases. Random forest has very high variance but on most missingness fractions with MCAR pattern, it ranks in $50\%$ of the cases on rank two or better. For MNAR its ranks getting better with higher missingness fractions, whereas this trends reverses for MNAR. The discriminative DL methods shows a very similar trend with tendency to less variance in its ranks. In most cases, the generative methods rank worst (GAIN) and on the middle ranks (VAE). However, with high missingess and when missing values are MNAR, they can perform better.

Similar to the fully observed training case (Section \ref{sec:results_experiment1_scenario1}), imputation on numerical columns yields a clearer ranking than for categorical missing values. The imputation methods $k$-NN and random forest rank best with a tendency of random forest to outperform $k$-NN, where random forest's variance is higher. The discriminative DL approach yields very similar performance to the $k$-NN for MCAR and MAR settings. In the more challenging MNAR setting it ranks slightly worse. For MCAR, mean/mode imputation ranks in almost all settings in $50\%$ of the cases between rank four and five, for MAR and MNAR between rank three and five. Again the generative methods rank in almost all settings in $75\%$ of the cases worse than rank for, where VAE tends to seldom rank the worst rank six.

Overall, the results of \textit{Scenario 1} (Figure \ref{fig:fully_observed_impute_rank_boxplot}) and \textit{Scenario 2} (Figure \ref{fig:corrupted_impute_rank_boxplot}) for numerical columns are quite similar. GAIN has become better in Scenario 2, although it still ranks worst. For categorical the ranks show generally higher variance. Most imputation methods getting worse with higher difficulty of the experimental setting, especially for MNAR, expect of mean/mode, which ranks better for MNAR. This effect is even clearer when training on incomplete data. In general, using simpler methods, such as $k$-NN or random forest achieves good to best results in most settings and cases.


\subsection{Experiment 2: Impact on Downstream Task}

In this experiment, we evaluate the downstream performance of each method in two scenarios: training on complete data and training on incomplete data. The plots are row-wise grouped by the type of downstream task (regression or classifiation). That is, in contrast to \textit{Experiment 1}, irrespective of the data type of the column which was imputed. \arndt{Beim Korrekturlesen nochmal drauf achten, dass das nicht irgendwo verdreht ist.}


\subsubsection{Scenario 1: Training on Complete Data}

\begin{figure}\centering
	\includegraphics[width=1\columnwidth]{fully_observed_downstream_boxplot.eps}
	\caption[Downstream Ranks - Fully Observed]{Impact on downstream task of the six imputation methods trained on complete data. We plot the imputation method's impact on downstream task against the missingness fraction. The columns of plots separate the changing missingness pattern, whereas the rows distinguish the results between categorical and numerical target columns. Overall, the classical ML methods and discriminative DL perform best.
    }
	\label{fig:fully_observed_downstream_boxplot}
\end{figure}

Figure \ref{fig:fully_observed_downstream_boxplot} visualizes the \textit{impact on downstream task} metric's version for \textit{Scenario 1}, that means the percent change of the imputation over the incomplete data relative to the downstream task performance on complete data (Equation \ref{eq:impact}). This metric is labeled \textit{Improvement} and represented on the y-axis.

Experiemental settings that caused undefined results in the first experiment were filtered out in the second experiment. This means in particular that we have about 33\% fewer results for GAIN in \textit{Scenario 1}.

In many conditions, imputation improves the downstream performance by up to 5\% or more. Unsurprisingly, the impact is mainly neglectable at the lowest missingness fraction of 1\%. At higher missingness fractions, we observe increasing improvements in all types of missingness patterns and downstream tasks. However, for categorical columns, downstream performance is degraded by imputation in mostly less than 25\% of cases.

For categorical columns, improvements are mostly in the 0\% to 5\% range, and in case of 50\% missing values the upper quartile reaches the 5\% to 10\% range. There are no clear differences between the results of the different imputation methods.

The situation is slightly different for regression tasks. Here, GAIN yields the least improvements downstream. Even the mean imputation is often more beneficial for the downstream performance. VAE is approximately on par with mean imputation. In contrast, the ML and the discriminative DL approaches do outperform the other methods in most settings. However, when the missing values are MNAR there is no clear advantage except that the discriminative DL is superior at 50\% missing values.

Overall, the classical ML methods and discriminative DL perform best. Again, mean/mode imputation yields medium good results. As the proportion of missing values increases, we observe increasing improvements of the in downstream performance in all types of missingness patterns, along with higher variance. In contrast to classification tasks, there are hardly any negative effects in regression tasks.


\subsubsection{Scenario 2: Training on Incomplete Data}

\begin{figure}\centering
	\includegraphics[width=1\columnwidth]{corrupted_downstream_boxplot.eps}

	\caption[Downstream Ranks - Corrupted]{Impact on downstream task of the six imputation methods trained on incomplete data. We plot the imputation method's impact on downstream task against the missingness fraction. The columns of plots separate the changing missingness pattern, whereas the rows distinguish the results between categorical and numerical target columns. In regression tasks, no considerable improvements are achieved. In classification tasks, in contrast, we observe slightly positive effects in some settings but negative effects predominate in the harder settings.
    }
	\label{fig:corrupted_downstream_boxplot}
\end{figure}

Figure \ref{fig:corrupted_downstream_boxplot} illustrates the \textit{impact on downstream task} metric's version for \textit{Scenario 2}, which means the percent change of the imputation over the incomplete data relative to the downstream task performance on incomplete data (Equation \ref{eq:impact_scenario2}). This metric is labeled \textit{Improvement} and represented on the y-axis. Here, the different scaling must be taken into account, i.e. the relative improvements are considerably smaller compared to the first scenario. One reason for this is the different basis for calculating the relative values (see sections \ref{sec:experiment_2} and \ref{sec:scenario_2}).

If weak positive effects seem to predominate in classification tasks at first, this tendency changes with increasing proportion of missing values (at 50\%) and increasing difficulty of the task (especially MNAR). The most negative results are produced by GAIN. However, the other methods also have major difficulties and often drift into negative ranges for the greater part as the tasks become more challenging.

In regression tasks, apart from a few outliers, there are no considerable improvements. At the highest proportion of missing values (50\%) and greatest difficulty (MNAR), the results of the generative methods (VAE and GAIN) even turn predominantly negative.

Here, too, the variance increases with higher fraciton. However, this time the results tend to worsen. In summary, no considerable improvements are achieved in regression tasks. In classification tasks, the picture is more mixed and the variance is higher. In some settings, there are slightly positive effects to note. In the difficult settings, however, the negative effects predominate. Again, $k$-NN and random forest appear to be the most robust methods overall.

\arndt{consider points below in discussion}
\felix{
\begin{itemize}
\item in the conclusion we should relate the performance to the training and prediction time and comment on the (large) differences - for AutoKeras, a lot more tuning is performed and we don't know whether it would have worked as well with less tuning.
\item we should highlight the limitations of our results (only one column, maybe too simple)
\item we should highlight the main findings; as far as i can see this would be: a) classical methods perform best (on these simple (?) data sets) and b) downstream performance is improved by X \% in Y \% of the experiments (we can use the quantiles in the boxplots) whereas when the imputation methods are trained on incomplete data, we cannot expect improvements.
\end{itemize}
}
\sebastian{
\begin{itemize}
\item Scenario 2: Training on Incomplete Data - Spätestens jetzt sollten wir mal ein wort darüber verlieren woran das liegen könnte: When eine category einfach häufiger ist, dann ist das meist ne recht guter guess. Das könnte dazu führen, dass die andere schlechter werden, weil die versuchen das intelligenter zu machen, aber einfach nicht genügend trainings daten haben um noch was sinnvolles zu lernen
\item Figure 3 - (Hbane wir ne idee warum classificaiotn auch schelcht wird und regression nicht?)
\end{itemize}
}

\subsection{Computational Complexity}
%
To measure the training and inference time, we use a sub-set of our experiments. Precisely, we use all data sets in the complete case scenario, MCAR missingness pattern, as well as the missingness fractions and imputation methods shown in Table \ref{tab:experiment_settings}. We then discarded values of the given missingness fraction in the training set and imputed those missing values. Again, we repeat the experiments three times to gain insights of the duration's variance. We use the wall-time when calling our framework's \code{fit} and \code{transform} methods (see Section \ref{sec:implementation} for details), which means that the training duration incorporates the hyperparameter optimization (see Section \ref{sec:HPO} for details). Table \ref{tab:time} presents the results.
%
\begin{table}
	\centering
	\label{tab:time}
	\begin{tabular}{lrr}
		\toprule
		Imputation Method &  Training Time &  Relative Standard Deviation \\
		\midrule
		Mean/Mode &       0.005009 &                     0.656056 \\
		$k$-NN &      40.961577 &                     0.243020 \\
		Random Forest &     225.513999 &                     0.118707 \\
		Discriminative DL &    6285.017741 &                     0.424011 \\
		VAE &      71.685278 &                     0.107189 \\
		GAIN &     874.657293 &                     0.299608 \\
		\bottomrule
	\end{tabular}
	\caption{Training time for each imputation method in seconds. Training time is the mean overall experimental settings, experiments, and scenarios. The data set's size skews the standard deviation heavily, which is why we first compute the relative standard deviation for each imputation method on each data set separately and then average over the data sets.\felix{for knn the training time should be almost 0, right? maybe it would be good to have the inference/prediction times as well, if we have that? kNN should perform much worse here, and the winner should be RF, i'm assuming}}
\end{table}
