%!TEX root = ../data-imputation.tex
\section{Results}
\label{sec:results}

In this section, we describe and visualize the results of our experiments. For the visualization we choose to consistently use boxplots for all four experiments/scenarios. These allow us to get a decent impression of the distribution of the results based on quantiles. In a line chart, in contrast, the confidence bands would overlap too much to derive meaningful interpretations. The plots' arrangement from left to right corresponds to the degree of difficulty increasing in this direction. This applies to the missingness patterns (MCAR, MAR, MNAR), each of which gets an own sub-plot, as well as the missingness percentages (small to larger), which are depicted as ticks on the x-axis. Due to mostly different results, we further distinguish the plots between regression and classification tasks. These are strongly imbalanced, with only 13\% of the results based on categorical columns (637) versus a clear majority of 87\% numerical columns (4293).


\subsection{Experiment 1: Imputation Quality}

In this experiment we evaluate the imputation performance of each method when training on fully observed data and incomplete data.

As described above, our goal was to provide a broad overview of the performance of different imputation methods on various data sets. It was difficult to compare the results across such heterogeneous data by their respective scores (F1/RMSE) on randomly sampled target columns, as the variance was much to high. Therefore, we decided to use ranks instead. On each experiemental condition (see Table \ref{tab:experiment_settings}), we calculate the rank of every imputer based on its mean performance over three repetions (different random train-test splits). Since we have six imputers, there are six ranks, with one being the best and six being the worst. For numerical columns the rank is based on RMSE where lower is better and for categorical columns the rank is based on the F1 score where higher is better. Imputation methods with equal performance are assigned the same rank.

If an imputation method did not work, it is ranked last. This is most relevant for GAIN, which failed in 273 out of 828 times in \textit{Experiment 1} - \textit{Scenario 1}, which amounts to 33\% of the cases. In \textit{Experiment 1} - \textit{Scenario 2}, however, the problem of GAIN often failing did not occur.


\subsubsection{Scenario 1: Training on Complete Data}
\label{sec:results_experiment1_scenario1}

\begin{figure}\centering
    \includegraphics[width=1\columnwidth]{fully_observed_impute_rank_boxplot.eps}
    \caption[Imputation Ranks - Fully Observed]{Imputation quality of the six imputation methods trained on complete data. We plot the imputation method's rank against the missingness fraction. The columns of plots separate the changing missingness pattern, whereas the rows distinguish the results between categorical and numerical target columns. ML methods and discriminative DL tend to perform best. From left to right, we observe more variance (larger boxes), indicating that the results are less clear. Especially for categorical columns with the MNAR pattern and higher missingness fractions, mean/mode seems to be a good alternative.
	}
	\label{fig:fully_observed_impute_rank_boxplot}
\end{figure}

Figure \ref{fig:fully_observed_impute_rank_boxplot} shows the aggregated ranks for imputation performance when imputation methods are trained on fully observed data.

When imputing categorical columns, there is no clear winner. However, the discriminative DL approach yields high imputation quality in most cases. In the MAR setting, also the methods random forest and $k$-NN are often among the first two to three ranks. With increasing complexity the DL based methods seem to improve. Notably mean/mode imputation compares favorably in many cases especially in the most complex MNAR setting. In most cases GAIN appears to perform worse than other methods. As mentioned above, we can attribute this poor performance in part to a large proportion (33\%) of failed runs of in the current experiment/scenario, in which case GAIN was ranked last. However, in the setting of highest complexity (MNAR with 50\% missing values) the results of GAIN seem to improve.

When imputing numerical columns the differences are more pronounced. Random forest is the only method with 75\% of values, i.e. data sets, among the first three ranks throughout all experimental conditions. However, the whiskers indicate that there are also some results ranging on the middle and even last ranks. In most settings, $k$-NN ranks second or third on $50\%$ of the data sets, with tendencies to better ranks. Discriminative DL is almost as strong. Mean/mode and VAE's ranks are similarly distributed over all ranks with tendency to worse performances. For easier settings, such as MCAR and MAR with smaller missingness fractions, mean/mode tends to be better than VAE. Again, GAIN performs worst, this time even more clearly.

To summarize, classical ML methods and discriminative DL perform best when imputing missing values. However, for categorical columns in the MNAR setting there is often no improvement over mode imputation. The generative methods are mostly outperformed even by mean/mode with VAE not as clearly as GAIN.


\subsubsection{Scenario 2: Training on Incomplete Data}


\begin{figure}\centering
    \includegraphics[width=1\columnwidth]{corrupted_impute_rank_boxplot.eps}

    \caption[Imputation Ranks - Corrupted]{Imputation quality of the six imputation methods trained on incomplete data. We plot the imputation method's rank against the missingness fraction. The columns of plots separate the changing missingness pattern, whereas the rows distinguish the results between categorical and numerical target columns. ML methods and discriminative DL tend to perform best. However, for categorical columns in the MNAR setting, there is often no improvement over mode imputation. \arndt{@Sebastian: ich finde, den letzten Satz kann man schon so schreiben im Bezug auf MNAR. Oder würdest du den weglassen?}
    }
	\label{fig:corrupted_impute_rank_boxplot}
\end{figure}

\autoref{fig:corrupted_impute_rank_boxplot} shows the imputation performance in \textit{Scenario 2}, i.e. when training on incomplete data. For categorical columns it is even harder to find clear patterns. With increasing task difficulty, the rank of the mean/mode imputation improves, though with higher variance. Interestingly, Mode imputation outperforms other methods especially in the most challenging settings (MNAR with 30\% and 50\% missing values). The methods $k$-NN and random forest compare favorably in the MCAR and MAR setting, but random forest has much higher variance. Discriminative DL yields similar performance in these settings, slightly worse on average and with lower variance than random forest. For MCAR and MAR these three methods tend to yield better performance than using Mean/Mode imputation, especially with an increasing fraction of missing values. The generative models tend to perform worst, especially GAIN.

Similar to the fully observed training case (Section \ref{sec:results_experiment1_scenario1}), imputation on numerical columns yields a clearer ranking than for categorical missing values. The imputation models $k$-NN and random forest yield best performances with a tendency of random forest to outperform $k$-NN, with larger variance of random forest. The Discriminative DL approach yields similar performance to the classical ML approaches in the MCAR and MAR settings. In the more challenging MNAR setting it performs slightly worse. Mean imputation and VAE perform comparably but worse than k-NN and Random Forest. GAIN falls on the last rank in most cases also in this setting.

Overall, the results of \textit{Scenario 1} (Figure \ref{fig:fully_observed_impute_rank_boxplot}) and \textit{Scenario 2} (Figure \ref{fig:corrupted_impute_rank_boxplot}) for numeric columns are quite similar. GAIN has become somewhat better in Scenario 2, although it still ranks worst. For categorical columns, the visualizations of the two scenarios are more pronounced in the incomplete training data setting. Whether imputation methods are trained on complete or incomplete data makes little difference in the ranks. The more difficult the task, depending on missingness pattern and fraction, the more ambiguous the results (higher variance). Classic ML methods like $k$-NN and random forest are generally a good choice. But also mean/mode yields relatively good results, especially when problems become more complex or the data is incomplete.


\subsection{Experiment 2: Impact on Downstream Task}

In this experiment we evaluate the downstream performance of each method when training on fully observed data and incomplete data.

In \textit{Scenario 1} we use the definition for \textit{impact on downstream task} from \autoref{eq:impact}. In contrast, for \textit{Scenario 2} we use the slightly different definition from \autoref{eq:impact_scenario2}. Both of theses metrics are labeled \textit{Improvement} and the values represented on the y-axis in \autoref{fig:fully_observed_downstream_boxplot} and \autoref{fig:corrupted_downstream_boxplot}.

In this experiment, we group the graphs by the type of downstream task. That is, in contrast to \textit{Experiment 1}, irrespective of the data type of the column that was imputed.

Experiemental settings that caused undefined results in the first experiment were filtered out in the second experiment. This means in particular that we have fewer results for GAIN in \textit{Scenario 1}.

When interpreting the results, it should be taken into account that, as described previously (Section \ref{sec:experiment_2}), we only imputed values in the test sets’ target column and measured the downstream improvement based on this.


\subsubsection{Scenario 1: Training on Complete Data}

\sebastian{GANZ WICHTIG: wir haben hier keine infos darüber, welche columns imputed wurden. Was wir sehen ist ne Gruppierung nach Downstream task!}
\arndt{@Sebastian: OK, habs oben eingefügt. Das gilt allgemein für Experiment 2, oder? War mir ehrlich gesagt nicht so richtig klar bisher. Da müssen wir beim Korrekturlesen nochmal drauf achten, dass das nicht irgendwo verdreht ist.}

\begin{figure}\centering
	\includegraphics[width=1\columnwidth]{fully_observed_downstream_boxplot.eps}

	\caption[Downstream Ranks - Fully Observed]{Impact on downstream task of the six imputation methods trained on complete data. We plot the imputation method's rank against the missingness fraction. The columns of plots separate the changing missingness pattern, whereas the rows distinguish the results between categorical and numerical target columns. Overall, the classical ML methods and discriminative DL perform best.
    }
	\label{fig:fully_observed_downstream_boxplot}
\end{figure}

Figure \ref{fig:fully_observed_downstream_boxplot} visualizes the \textit{impact on downstream task} metric, i.e. the percent change of the imputation over the incomplete data relative to the baseline (Equation \ref{eq:impact}), in \textit{Scenario 1}.

The impact of imputation seems mostly negligible if the proportion of missing values is only 1\%. However, for numeric columns, there are some outliers in the 20\% to 40\% improvement range with the MCAR and MAR patterns at 1\% missing values. With an increasing proportion of missing values we observe increasing improvements in the downstream performance in all types of missingness patterns. However, there are also some cases where downstream performance is degraded by imputation, which is mainly the case for categorical columns.

For categorical columns, improvements are mostly in the 0\% to 5\% range, and in case of 50\% missing values the upper quartile reaches the 5\% to 10\% range. The whiskers even indicate performance increases which go significantly beyond. Measured by the median, as an overall aggregation, random forest yields the best results for categorical columns. GAIN performs often similarly well or even better but also has more negative fluctuations.

The situation is different for regression tasks. Here, GAIN clearly yields the least improvements downstream. Even the Mean imputation is oftentimes significantly more beneficial for the downstream tasks. The VAE results are approximately on par with Mean imputation. In contrast, the classical ML and the discriminative DL approaches do outperform the other methods in most settings. However, when the missing values are MNAR there is no clear advantage except that the discriminative DL is superior at 50\% missing values.

Overall, the classical ML methods and discriminative DL perform best. Again, mean/mode imputation yields medium good results. As the proportion of missing values increases, we observe increasing improvements of the in downstream performance in all types of missingness patterns, along with higher variance. In contrast to classification tasks, there are hardly any negative effects in regression tasks.


\subsubsection{Scenario 2: Training on Incomplete Data}

\begin{figure}\centering
	\includegraphics[width=1\columnwidth]{corrupted_downstream_boxplot.eps}

	\caption[Downstream Ranks - Corrupted]{Impact on downstream task of the six imputation methods trained on incomplete data. We plot the imputation method's rank against the missingness fraction. The columns of plots separate the changing missingness pattern, whereas the rows distinguish the results between categorical and numerical target columns. In regression tasks, no considerable improvements are achieved. In classification tasks, in contrast, we observe slightly positive effects in some settings but negative effects predominate in the harder settings.
    }
	\label{fig:corrupted_downstream_boxplot}
\end{figure}

Figure \ref{fig:corrupted_downstream_boxplot} illustrates the \textit{impact on downstream task} (Equation \ref{eq:impact_scenario2}) in \textit{Scenario 2} (training on incomplete/imputed data). Here, the different scaling of the y-axis must be taken into account, i.e. the relative improvements are significantly smaller compared to the first scenario. One reason for this is the different basis for calculating the relative values (see sections \ref{sec:experiment_2} and \ref{sec:scenario_2}).

If weak positive effects seem to predominate in classification tasks at first, this tendency changes with increasing proportion of missing values (at 50\%) and increasing difficulty of the task (especially MNAR). The most negative results are produced by GAIN. However, the other methods also have major difficulties and often drift into negative ranges for the greater part as the tasks become more challenging.

In regression tasks, apart from a few outliers, there are no significant improvements. At the highest proportion of missing values (50\%) and greatest difficulty (MNAR), the results of the generative methods (VAE and GAIN) even turn predominantly negative.

Here, too, the variance increases with higher fraciton. However, this time the results tend to worsen. In summary, no considerable improvements are achieved in regression tasks. In classification tasks, the picture is more mixed and the variance is higher. In some settings, there are slightly positive effects to note. In the difficult settings, however, the negative effects predominate. Again, $k$-NN and random forest appear to be the most robust methods overall.

\arndt{consider points below in discussion}
\felix{
\begin{itemize}
\item in the conclusion we should relate the performance to the training and prediction time and comment on the (large) differences - for AutoKeras, a lot more tuning is performed and we don't know whether it would have worked as well with less tuning.
\item we should highlight the limitations of our results (only one column, maybe too simple)
\item we should highlight the main findings; as far as i can see this would be: a) classical methods perform best (on these simple (?) data sets) and b) downstream performance is improved by X \% in Y \% of the experiments (we can use the quantiles in the boxplots) whereas when the imputation methods are trained on incomplete data, we cannot expect improvements.
\end{itemize}
}
\sebastian{
\begin{itemize}
\item Scenario 2: Training on Incomplete Data - Spätestens jetzt sollten wir mal ein wort darüber verlieren woran das liegen könnte: When eine category einfach häufiger ist, dann ist das meist ne recht guter guess. Das könnte dazu führen, dass die andere schlechter werden, weil die versuchen das intelligenter zu machen, aber einfach nicht genügend trainings daten haben um noch was sinnvolles zu lernen
\item Figure 3 - (Hbane wir ne idee warum classificaiotn auch schelcht wird und regression nicht?)
\end{itemize}
}

\subsection{Training Time}

\begin{table}
	\centering
	\label{tab:time}
	\begin{tabular}{lrr}
		\toprule
		Imputation Method &  Training Time &  Relative Standard Deviation \\
		\midrule
		Mean/Mode &       0.005009 &                     0.656056 \\
		$k$-NN &      40.961577 &                     0.243020 \\
		Random Forest &     225.513999 &                     0.118707 \\
		Discriminative DL &    6285.017741 &                     0.424011 \\
		VAE &      71.685278 &                     0.107189 \\
		GAIN &     874.657293 &                     0.299608 \\
		\bottomrule
	\end{tabular}
	\caption{Training time for each imputation method in seconds. Training time is the mean overall experimental settings, experiments, and scenarios. The data set's size skews the standard deviation heavily, which is why we first compute the relative standard deviation for each imputation method on each data set separately and then average over the data sets.\felix{for knn the training time should be almost 0, right? maybe it would be good to have the inference/prediction times as well, if we have that? kNN should perform much worse here, and the winner should be RF, i'm assuming}}
\end{table}
