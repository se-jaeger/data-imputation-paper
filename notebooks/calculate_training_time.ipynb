{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adjustable-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from data_imputation_paper.experiment import _recursive_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southern-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_a = str(Path(f\"../data/experiments/fully_observed\") / \"**\" / \"MCAR\" / \"**\" / \"single_all\" / \"elapsed_train_time_*.json\")\n",
    "glob_b = str(Path(f\"../data/experiments/corrupted\") / \"**\" / \"MCAR\" / \"**\" / \"single_all\" / \"elapsed_train_time_*.json\")\n",
    "training_time_files = [*glob.glob(glob_a, recursive=True), *glob.glob(glob_b, recursive=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stuffed-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"imputer\", \"task\", \"type\", \"fraction\"]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for path in training_time_files:\n",
    "    df = pd.read_json(path, orient=\"index\").T.reset_index(drop=True)\n",
    "    df[column_names] = _recursive_split(path)[4:-2]\n",
    "    dfs.append(df)\n",
    "\n",
    "training_time_all = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "former-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time = training_time_all.drop(\"std\", axis=1)\n",
    "training_time = training_time.rename(columns={\"mean\": \"training_time\"})\n",
    "training_time = training_time.replace({\n",
    "    \"ModeImputer\": \"Mean/Mode\",\n",
    "    \"KNNImputer\": \"$k$-NN\",\n",
    "    \"ForestImputer\": \"Random Forest\",\n",
    "    \"AutoKerasImputer\": \"Discriminative DL\",\n",
    "    \"VAEImputer\": \"VAE\",\n",
    "    \"GAINImputer\": \"GAIN\"    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "athletic-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate mean and relative std for each imputer and task\n",
    "training_time_grouped = training_time.groupby([\"imputer\", \"task\"]).agg([\"mean\", \"std\"])\n",
    "training_time_grouped.columns = [\"mean\", \"std\"]\n",
    "training_time_grouped[\"rel std\"] = training_time_grouped.loc[:, \"std\"] /  training_time_grouped.loc[:, \"mean\"]\n",
    "\n",
    "# Then average over all data sets. This leads to relativ sd that is less dependant on the data set size.\n",
    "training_time_grouped = training_time_grouped.groupby(\"imputer\").agg([\"mean\", \"std\"])\n",
    "training_time_grouped = training_time_grouped.loc[:, [(\"mean\", \"mean\"), (\"rel std\", \"mean\")]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-india",
   "metadata": {},
   "source": [
    "# Predict Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intense-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_time_files = glob.glob(str(Path(f\"../data/experiments/time_measure_predict\") / \"**\" / \"MCAR\" / \"**\" / \"single_single\" / \"elapsed_train_time_*.json\"), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appointed-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"imputer\", \"task\", \"type\", \"fraction\"]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for path in predict_time_files:\n",
    "    df = pd.read_json(path, orient=\"index\").T.reset_index(drop=True)\n",
    "    df[column_names] = _recursive_split(path)[4:-2]\n",
    "    dfs.append(df)\n",
    "\n",
    "predict_time_all = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "manual-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_time = predict_time_all.drop(\"std\", axis=1)\n",
    "predict_time = predict_time.rename(columns={\"mean\": \"predict_time\"})\n",
    "predict_time = predict_time.replace({\n",
    "    \"ModeImputer\": \"Mean/Mode\",\n",
    "    \"KNNImputer\": \"$k$-NN\",\n",
    "    \"ForestImputer\": \"Random Forest\",\n",
    "    \"AutoKerasImputer\": \"Discriminative DL\",\n",
    "    \"VAEImputer\": \"VAE\",\n",
    "    \"GAINImputer\": \"GAIN\"    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unnecessary-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate mean and relative std for each imputer and task\n",
    "predict_time_grouped = predict_time.groupby([\"imputer\", \"task\"]).agg([\"mean\", \"std\"])\n",
    "predict_time_grouped.columns = [\"mean\", \"std\"]\n",
    "predict_time_grouped[\"rel std\"] = predict_time_grouped.loc[:, \"std\"] /  predict_time_grouped.loc[:, \"mean\"]\n",
    "\n",
    "# Then average over all data sets. This leads to relativ sd that is less dependant on the data set size.\n",
    "predict_time_grouped = predict_time_grouped.groupby(\"imputer\").agg([\"mean\", \"std\"])\n",
    "predict_time_grouped = predict_time_grouped.loc[:, [(\"mean\", \"mean\"), (\"rel std\", \"mean\")]]\n",
    "predict_time_grouped = predict_time_grouped.loc[[\"Mean/Mode\", \"$k$-NN\", \"Random Forest\", \"Discriminative DL\", \"VAE\", \"GAIN\"],:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "defensive-banner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>imputer</th>\n",
       "      <th>mean</th>\n",
       "      <th>rel std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean/Mode</td>\n",
       "      <td>0.029195</td>\n",
       "      <td>0.171139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$k$-NN</td>\n",
       "      <td>7.018330</td>\n",
       "      <td>0.602026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>44.047614</td>\n",
       "      <td>0.236052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Discriminative DL</td>\n",
       "      <td>440.388738</td>\n",
       "      <td>0.210621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VAE</td>\n",
       "      <td>11.214830</td>\n",
       "      <td>0.084604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GAIN</td>\n",
       "      <td>137.965780</td>\n",
       "      <td>0.083426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             imputer        mean   rel std\n",
       "                            mean      mean\n",
       "0          Mean/Mode    0.029195  0.171139\n",
       "1             $k$-NN    7.018330  0.602026\n",
       "2      Random Forest   44.047614  0.236052\n",
       "3  Discriminative DL  440.388738  0.210621\n",
       "4                VAE   11.214830  0.084604\n",
       "5               GAIN  137.965780  0.083426"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_time_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-hawaiian",
   "metadata": {},
   "source": [
    "# Latex Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vital-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Imputation Method</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Training</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Inference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Mean Duration</th>\n",
       "      <th>Rel. SD</th>\n",
       "      <th>Mean Duration</th>\n",
       "      <th>Rel. SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean/Mode</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.550878</td>\n",
       "      <td>0.029195</td>\n",
       "      <td>0.171139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$k$-NN</td>\n",
       "      <td>41.204365</td>\n",
       "      <td>0.253716</td>\n",
       "      <td>7.01833</td>\n",
       "      <td>0.602026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>226.076551</td>\n",
       "      <td>0.119295</td>\n",
       "      <td>44.047614</td>\n",
       "      <td>0.236052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Discriminative DL</td>\n",
       "      <td>6275.019244</td>\n",
       "      <td>0.40505</td>\n",
       "      <td>440.388738</td>\n",
       "      <td>0.210621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VAE</td>\n",
       "      <td>71.095282</td>\n",
       "      <td>0.098795</td>\n",
       "      <td>11.21483</td>\n",
       "      <td>0.084604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GAIN</td>\n",
       "      <td>878.058286</td>\n",
       "      <td>0.311553</td>\n",
       "      <td>137.96578</td>\n",
       "      <td>0.083426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Imputation Method      Training               Inference          \n",
       "                     Mean Duration   Rel. SD Mean Duration   Rel. SD\n",
       "0          Mean/Mode      0.005277  0.550878      0.029195  0.171139\n",
       "1             $k$-NN     41.204365  0.253716       7.01833  0.602026\n",
       "2      Random Forest    226.076551  0.119295     44.047614  0.236052\n",
       "3  Discriminative DL   6275.019244   0.40505    440.388738  0.210621\n",
       "4                VAE     71.095282  0.098795      11.21483  0.084604\n",
       "5               GAIN    878.058286  0.311553     137.96578  0.083426"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_latex = training_time_grouped.loc[[\"Mean/Mode\", \"$k$-NN\", \"Random Forest\", \"Discriminative DL\", \"VAE\", \"GAIN\"],:].reset_index()\n",
    "table_latex[(\"a\", \"mean\")] = predict_time_grouped[(\"mean\", \"mean\")]\n",
    "table_latex[(\"a\", \"std\")] = predict_time_grouped[(\"rel std\", \"mean\")]\n",
    "table_latex = pd.DataFrame(table_latex.values, columns=pd.MultiIndex.from_tuples([(\"Imputation Method\", \"\"), (\"Training\", \"Mean Duration\"), (\"Training\", \"Rel. SD\"), (\"Inference\", \"Mean Duration\"), (\"Inference\", \"Rel. SD\")]))\n",
    "table_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "educational-spain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Training and inference duration for each imputation method in seconds. We use the wall-time to measure the durations for training including hyperparameter optimization and inference for MCAR missingness pattern and all missingness fractions shown in Table TODO. Because training and inference durations depend heavily on the data set, we first average all measurements for imputation method and data set combinations and calculate the standard deviation relatives relative to there mean durations. Second, we average both mean durations and relative standard devaition for the imputation methods. Abbreviations: Rel. SD means Relative Standard Deviation.}\n",
      "\\label{tab:time}\n",
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "Imputation Method & \\multicolumn{2}{l}{Training} & \\multicolumn{2}{l}{Inference} \\\\\n",
      "                  & Mean Duration &   Rel. SD & Mean Duration &   Rel. SD \\\\\n",
      "\\midrule\n",
      "        Mean/Mode &      0.005277 &  0.550878 &      0.029195 &  0.171139 \\\\\n",
      "           $k$-NN &     41.204365 &  0.253716 &       7.01833 &  0.602026 \\\\\n",
      "    Random Forest &    226.076551 &  0.119295 &     44.047614 &  0.236052 \\\\\n",
      "Discriminative DL &   6275.019244 &   0.40505 &    440.388738 &  0.210621 \\\\\n",
      "              VAE &     71.095282 &  0.098795 &      11.21483 &  0.084604 \\\\\n",
      "             GAIN &    878.058286 &  0.311553 &     137.96578 &  0.083426 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    table_latex.to_latex(\n",
    "        caption=\"Training and inference duration for each imputation method in seconds. We use the wall-time to measure the durations for training including hyperparameter optimization and inference for MCAR missingness pattern and all missingness fractions shown in Table TODO. Because training and inference durations depend heavily on the data set, we first average all measurements for imputation method and data set combinations and calculate the standard deviation relatives relative to there mean durations. Second, we average both mean durations and relative standard devaition for the imputation methods. Abbreviations: Rel. SD means Relative Standard Deviation.\",\n",
    "        label=\"tab:time\",\n",
    "        index=False,\n",
    "        escape=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-myanmar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-inflation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
